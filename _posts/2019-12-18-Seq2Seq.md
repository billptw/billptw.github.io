---
layout: post
title: Sequence to Sequence Learning with Neural Networks
---

Let's start the blogging adventure with a historic model: the encoder-decoder architecture in sequence-to-sequence modeling [(Sutskever et al., 2014)](https://arxiv.org/pdf/1409.3215.pdf). In this post, we will walk through the motivations behind this model, and probe further into the model architecture.

## Motivation

Sequence-to-seqence modeling can be applied to any task involving sequences, such as speech recognition, question answering, and neural machine translation (NMT). Let's start with the motivating example of translating the English sentence "I am walking" into German (which yields "Ich laufe"). To convert each input sentence to a form for neural networks to understand, we break down the sentence into tokens, where say word-level tokenization means the sequence "I am walking" is converted into three tokens 'I', 'am', and 'walking. The tokens are then converted into word embedding form, a many-dimensional vector where words with similar semantics are closer in distance to one another. Given that our input sequence contains three tokens, how do we feed it into the neural network to generate our desired output of two tokens?

Herein lies the problem with using a feedforward/recurrent network in NMT- the input and output dimensions of a network has to be fixed, whereas input and output sentences have varying lengths. A possible but highly impractical workaround involves training $$n \times m$$ networks for all desired combinations of input size of $$n$$ tokens and output size $$m$$, where one can see that to translate sentences of $$\eq 10$$ words we require an ensemble of $$100$$ neural models!


## Model

In the motivating example, we were thinking about how to fit sequences with varying lengths into the network. To bypass the **spatial** constraint of having a fixed input dimension in our network, we instead process it **temporally**- we first pass the input sequence token-by-token into an encoder network to generate an encoded vector of the input sentence, which is then fed into a decoder network to generate the translated network.

![Seq2Seq Model](/images/seq2seq.png)
Source: [Medium post](https://medium.com/@devnag/seq2seq-the-clown-car-of-deep-learning-f88e1204dac3) by Dev Nag

