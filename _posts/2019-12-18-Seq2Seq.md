---
layout: pos:
title: Sequence to Sequence Learning with Neural Networks
---

Let's start the blogging adventure with a historic model: the encoder-decoder architecture in sequence-to-sequence modeling [(Sutskever et al., 2014)](https://arxiv.org/pdf/1409.3215.pdf). In this post, we will walk through the motivations behind this model, and probe further into the model architecture.

## Motivation

Neural machine translation (NMT) is the task of the converting of a sequence from one language to another using a neural network. Let's start with the task of translating the English sentence "I am walking" into German (which yields "Ich laufe"). To convert each input sentence to a form for neural networks to understand, we break down the sentence into tokens, where say word-level tokenization means the sequence "I am walking" is converted into three tokens 'I', 'am', and 'walking. The tokens are then converted into word embedding form, a many-dimensional vector where words with similar semantics are closer in distance to one another.



Herein lies the first problem with using a feedforward network in NMT- given that the lenghts 

The tokens are then concatenated and fed into the neural network, where the weights and activations of the neurons act on the embedding representation of the tokens to produce the tokens 'Ich' and 'laufe' based on the probability of 

## Model

![Seq2Seq Model](/images/seq2seq.png)


