---
layout: post
title: Sequence to Sequence Learning with Neural Networks
---

Let's start the blogging adventure with a historic model: the encoder-decoder architecture in sequence-to-sequence modeling [(Sutskever et al., 2014)](https://arxiv.org/pdf/1409.3215.pdf). In this post, we will walk through the motivations behind this model, and probe further into the model architecture.

## Motivation

Neural machine translation (NMT) is the task of the converting of a sequence from one language to another using a neural network. Let's start with the task of translating the English sentence "I am walking" into German (which yields "Ich laufe"). To convert each input sentence to a form for neural networks to understand, we break down the sentence into tokens, where say word-level tokenization means the sequence "I am walking" is converted into three tokens 'I', 'am', and 'walking. The tokens are then converted into word embedding form, a many-dimensional vector where words with similar semantics are closer in distance to one another. Given that our input sequence contains three tokens, how do we feed it into the neural network to generate our desired output of two tokens?

Herein lies the problem with using a feedforward/recurrent network in NMT- the input and output dimensions of a network has to be fixed, whereas input and output sentences have varying lengths. A possible but highly impractical workaround involves training $$n \multiply m$$ networks for all desired combinations of input size of $$n$$ tokens and output size $$m$$, where one can see that to translate sentences of up to 10 words we require an ensemble of 100 neural models!


## Model

![Seq2Seq Model](/images/seq2seq.png)


